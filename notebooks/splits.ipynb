{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset Splitting Strategies\n",
    "\n",
    "Here we present various data splitting strategies for our datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "import dask.utils\n",
    "import numpy as np\n",
    "from numpy.random import SeedSequence\n",
    "\n",
    "import dask.bag as db"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "entropy = 0x87351080e25cb0fad77a44a3be03b491\n",
    "seed = np.random.SeedSequence(entropy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Union, Iterable\n",
    "\n",
    "from matchms import Spectrum\n",
    "from matchms.importing import load_from_msp\n",
    "from matchms.exporting import save_as_msp\n",
    "\n",
    "\n",
    "def extract_spectrum(spectrum: Spectrum) -> Dict[str, Any]:\n",
    "    return {\n",
    "        'mz': np.asarray(spectrum.peaks.mz),\n",
    "        'intensity': np.asarray(spectrum.peaks.intensities),\n",
    "        **spectrum.metadata\n",
    "    }\n",
    "\n",
    "\n",
    "def read_msp(files: Union[str, List[str]]) -> db.Bag:\n",
    "    spectra = db.from_sequence(files).map(load_from_msp).flatten()\n",
    "    spectra = spectra.map(extract_spectrum)\n",
    "    return spectra\n",
    "\n",
    "def write_msp_splits(splits: Iterable[db.Bag], filenames: Iterable[str]) -> None:\n",
    "    for split, filename in zip(splits, filenames):\n",
    "        save_as_msp(iter(split), filename)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "nist = read_msp('data/NIST_EI_MS.msp')\n",
    "mona = read_msp('data/MONA.msp')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Random Split\n",
    "\n",
    "The most trivial splitting strategy based on pure randomness."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from operator import getitem\n",
    "from dask.base import tokenize\n",
    "from dask.highlevelgraph import HighLevelGraph\n",
    "\n",
    "def category_split(bag: db.Bag, categories):\n",
    "    ncategories = len(np.unique(categories))\n",
    "\n",
    "    token = tokenize(bag, categories)\n",
    "    name = 'split-' + token\n",
    "    layer = {\n",
    "        (name, i): (getitem, ) for i in range(bag.npartitions)\n",
    "    }\n",
    "\n",
    "    out = []\n",
    "    for i in range(ncategories):\n",
    "        name2 = f'category-split-{i}-{token}'\n",
    "        dask2 = {\n",
    "            (name2, j): (getitem, (bag.name, i), i) for j in range(bag.npartitions)\n",
    "        }\n",
    "        graph = HighLevelGraph.from_collections(name2, dask2, dependencies=[bag])\n",
    "        out.append(db.Bag(graph, name2, bag.npartitions))\n",
    "    return out\n",
    "\n",
    "\n",
    "def cat_split(bag: db.Bag, categories: db.Bag):\n",
    "    bag.map_partitions(getitem, )\n",
    "\n",
    "\n",
    "def random_split(data, path, random_state = None):\n",
    "    index = dask.utils.pseudorandom(len(data), [.7, .15, .15], random_state=random_state)\n",
    "\n",
    "    splits = spectra.random_split([.7, .15, .15], seed)\n",
    "\n",
    "    write_msp_splits(splits, ['train.msp', 'test.msp', 'validation.msp'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "random_split(nist, 'nist_random_split', seed)\n",
    "random_split(mona, 'mona_random_split', seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split A\n",
    "\n",
    "Take the first occurrence of inchikey and put it in the training split, the rest will fall into the test split. This split will be focused on recognizing known compounds but with previously unseen fragmentation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def duplicated(spectra):\n",
    "    spectra['inchikey'].duplicated()\n",
    "\n",
    "def unique_split(data, path, key = 'inchikey'):\n",
    "    splits = category_split(data, category)\n",
    "\n",
    "    write_msp_splits(splits, [''])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "unique_split(nist, 'nist_unique_split')\n",
    "unique_split(mona, 'mona_unique_split')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split B"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "[0, 0, 5, 5]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import getitem\n",
    "\n",
    "x = db.from_sequence(range(10), npartitions=2)\n",
    "y = db.from_sequence([0,0,0,0,0,1,1,1,1,1], npartitions=2)\n",
    "\n",
    "x = x.map_partitions(lambda a: [a[y == i] for i in range(2)])\n",
    "x.compute()\n",
    "#x.map_partitions(getitem, y).compute()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 4, 6, 8]\n",
      "[1, 3, 5, 7, 9]\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from dask.bag import Bag\n",
    "from dask.highlevelgraph import HighLevelGraph\n",
    "\n",
    "def unique(seq: Bag) -> Bag:\n",
    "    return seq.fold(lambda acc, item: acc | {item}, set.union, initial=set())\n",
    "\n",
    "\n",
    "def _select(seq, cat, sel):\n",
    "    return list(a for a, b in zip(seq, cat) if b == sel)\n",
    "\n",
    "def partition(bag: Bag, cat: Bag) -> List[Bag]:\n",
    "    assert (bag.npartitions == cat.npartitions)\n",
    "\n",
    "    out = []\n",
    "    name = 'partition-' + tokenize(bag, cat)\n",
    "\n",
    "    for sel in unique(cat).compute():\n",
    "        name2 = f'{name}-{sel}'\n",
    "        dsk = {\n",
    "            (name2, i): (_select, (bag.name, i), (cat.name, i), sel) for i in range(bag.npartitions)\n",
    "        }\n",
    "        graph = HighLevelGraph.from_collections(name2, dsk, dependencies=[bag, cat])\n",
    "        out.append(Bag(graph, name2, bag.npartitions))\n",
    "    return out\n",
    "\n",
    "\n",
    "a = db.from_sequence(range(10), npartitions=2)\n",
    "b = db.from_sequence([0,1,0,1,0,1,0,1,0,1], npartitions=2)\n",
    "\n",
    "for p in partition(a, b):\n",
    "    print(p.compute())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "data": {
      "text/plain": "[[0], [], [], [], [], [], [1], [], [], []]"
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _foo(seq, cat, fact):\n",
    "    return [[a for a, b in zip(seq, cat) if b == f] for f in fact]\n",
    "\n",
    "def _unique(seq: Bag) -> Bag:\n",
    "    return seq.fold(lambda acc, item: acc | {item}, set.union, initial=set())\n",
    "\n",
    "def _split2(sequence: Bag, categories: Bag):\n",
    "    name = 'split-' + tokenize(sequence, categories)\n",
    "    fact = _unique(categories).compute()\n",
    "\n",
    "    g1 = {\n",
    "        (name, i): (_foo, (sequence.name, i), (categories.name, i), fact) for i in range(sequence.npartitions)\n",
    "    }\n",
    "\n",
    "    return Bag(g1, name, npartitions=sequence.npartitions)\n",
    "    #[Bag(g2, n2, sequence.npartitions) for f in fact]\n",
    "\n",
    "_split2(a, b).compute()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "data": {
      "text/plain": "[[0, 2, 4, 6, 8], [1, 3, 5, 7, 9]]"
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _split(seq, cat):\n",
    "    return [[a for a, b in zip(seq, cat) if b == c] for c in set(cat)]\n",
    "\n",
    "_foo(list(range(10)), [0,1,0,1,0,1,0,1,0,1], set([0,1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "[0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4]"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsk = {('x', 0): (range, 5),\n",
    "       ('x', 1): (range, 5),\n",
    "       ('x', 2): (range, 5)}\n",
    "db.Bag(dsk, 'x', npartitions=3).compute()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "from matchms.importing import load_from_msp\n",
    "\n",
    "def random_split(data, frac, seed = None):\n",
    "    data = np.asarray(data)\n",
    "    frac = np.asarray(frac)\n",
    "\n",
    "    if not numpy.allclose(sum(frac), 1):\n",
    "        raise ValueError('frac should sum to 1')\n",
    "\n",
    "    n = len(spectra)\n",
    "    x = default_rng(seed).random(n)\n",
    "    cp = numpy.cumsum(numpy.append([0], frac))\n",
    "    indices = np.empty(n, dtype=numpy.int64)\n",
    "\n",
    "    for i, (low, high) in enumerate(zip(cp[:-1], cp[1:])):\n",
    "        indices[(x >= low) & (x < high)] = i\n",
    "\n",
    "    return [data[(x >= low) & (x < high)] for low, high in zip(cp[:-1], cp[1:])]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66759752 0.62637313 0.02760391 0.29911756 0.68579046 0.58472615\n",
      " 0.35768609 0.30958816 0.8702187  0.41426965]\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import default_rng\n",
    "\n",
    "rng = default_rng()\n",
    "vals = rng.random(10)\n",
    "print(vals)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional, Generator, Dict\n",
    "\n",
    "import matchms.filtering\n",
    "import matchms.importing\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from spec2vec import SpectrumDocument\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "def process_spectrum(spectrum: Optional[matchms.Spectrum], n_required_peaks: Optional[int] = 10,\n",
    "                     n_max_peaks: Optional[int] = None, min_relative_intensity: Optional[int] = None) -> Optional[\n",
    "    matchms.Spectrum]:\n",
    "    spectrum = matchms.filtering.select_by_mz(spectrum, mz_from=0, mz_to=1000)\n",
    "    spectrum = matchms.filtering.normalize_intensities(spectrum)\n",
    "\n",
    "    if n_required_peaks is not None:\n",
    "        spectrum = matchms.filtering.require_minimum_number_of_peaks(spectrum, n_required=n_required_peaks)\n",
    "    if n_max_peaks is not None:\n",
    "        spectrum = matchms.filtering.reduce_to_number_of_peaks(spectrum, n_max=n_max_peaks)\n",
    "    if min_relative_intensity is not None:\n",
    "        spectrum = matchms.filtering.select_by_relative_intensity(spectrum, intensity_from=min_relative_intensity)\n",
    "    return spectrum\n",
    "\n",
    "\n",
    "def load_msp_documents(filename: str, n_max_peaks: Optional[int] = None) -> Generator[SpectrumDocument, None, None]:\n",
    "    spectra = (process_spectrum(spectrum, n_max_peaks=n_max_peaks) for spectrum in\n",
    "               matchms.importing.load_from_msp(filename))\n",
    "    spectra = (SpectrumDocument(spectrum, n_decimals=0) for spectrum in spectra if spectrum is not None)\n",
    "    return spectra\n",
    "\n",
    "\n",
    "class HuggingfaceDataset(Dataset):\n",
    "    def __init__(self, filename: str, vocabulary: Dict, max_length: int = 256, include_intensity: bool = False,\n",
    "                 quadratic_bins: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.spectrum_documents = list(load_msp_documents(filename))\n",
    "        self.vocabulary = vocabulary\n",
    "        self.max_length = max_length\n",
    "        self.include_intensity = include_intensity\n",
    "\n",
    "        if quadratic_bins:\n",
    "            self.bins = ((numpy.arange(max_length) ** 2) / ((max_length - 1) ** 2))[::-1]\n",
    "        else:\n",
    "            self.bins = (numpy.arange(max_length) / (max_length - 1))[::-1]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.spectrum_documents)\n",
    "\n",
    "    def __getitem__(self, index) -> Dict[str, Tensor]:\n",
    "        document = self.spectrum_documents[index]\n",
    "        indices = torch.argsort(document.peaks.intensities)[::-1]\n",
    "        indices = indices[:self.max_length]\n",
    "\n",
    "        x = self.encode_spectrum(torch.asarray(document.words)[indices])\n",
    "        x_padded = torch.cat([x, torch.zeros(self.max_length - len(x), dtype=torch.int)])\n",
    "\n",
    "        attention_mask = torch.zeros(self.max_length, dtype=torch.int)\n",
    "        attention_mask[:len(x)] = 1\n",
    "\n",
    "        result = {\"input_ids\": x_padded, \"attention_mask\": attention_mask}\n",
    "\n",
    "        if self.include_intensity:\n",
    "            position_ids = torch.asarray(numpy.digitize(document.peaks.intensities[indices], self.bins, right=False))\n",
    "            padding = torch.zeros(self.max_length - len(position_ids), dtype=torch.int) + self.max_length - 1\n",
    "            result[\"position_ids\"] = torch.cat([position_ids, padding])\n",
    "        return result\n",
    "\n",
    "    def size(self) -> int:\n",
    "        return len(self.vocabulary)\n",
    "\n",
    "    def encode_peak(self, peak) -> Tensor:\n",
    "        return torch.tensor(self.vocabulary[peak], dtype=torch.int)\n",
    "\n",
    "    def encode_spectrum(self, spectrum) -> Tensor:\n",
    "        encoded_peaks = [self.encode_peak(peak) for peak in spectrum if peak in self.vocabulary]\n",
    "        return torch.tensor(encoded_peaks, dtype=torch.int)\n",
    "\n",
    "\n",
    "class GenerativeDataset(Dataset):\n",
    "    def __init__(self, filename: str, vocabulary: Dict, onehot: bool = True, include_intensity: bool = True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.spectrum_documents = list(load_msp_documents(filename))\n",
    "        self.vocabulary = vocabulary\n",
    "        self.onehot = onehot\n",
    "        self.include_intensity = include_intensity\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.spectrum_documents)\n",
    "\n",
    "    def __getitem__(self, index) -> Tuple[Tensor, Tensor]:\n",
    "        indices = numpy.argsort(self.spectrum_documents[index].peaks.intensities)[::-1]\n",
    "        x_indices = indices[:-1]\n",
    "        y_indices = indices[1:]\n",
    "\n",
    "        intensity = self.spectrum_documents[index].peaks.intensities[x_indices]\n",
    "\n",
    "        x = self.encode_spectrum(self.spectrum_documents[index].words[x_indices], onehot=self.onehot)\n",
    "        y = self.encode_spectrum(self.spectrum_documents[index].words[y_indices], onehot=False)\n",
    "\n",
    "        if self.include_intensity:\n",
    "            intensity = self.spectrum_documents[index].peaks.intensities[x_indices]\n",
    "            x = self.encode_spec_intens(self.spectrum_documents[index].words[x_indices], intensity)\n",
    "        else:\n",
    "            x = self.encode_spectrum(self.spectrum_documents[index].words[x_indices])\n",
    "        return x, y\n",
    "\n",
    "    def size(self) -> int:\n",
    "        return len(self.vocabulary)\n",
    "\n",
    "    def encode_peak(self, peak) -> Tensor:\n",
    "        return torch.tensor(self.vocabulary[peak], dtype=torch.int)\n",
    "\n",
    "    def encode_spectrum(self, spectrum, intensity, onehot: bool):\n",
    "        encoded_peaks = [self.encode_peak(peak) for peak in spectrum if peak in self.vocabulary]\n",
    "\n",
    "        if onehot:\n",
    "            encoded_peaks = [F.one_hot(p, num_classes=len(self.vocabulary)) for p in encoded_peaks]\n",
    "        return torch.stack(encoded_peaks)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "spectra = load_msp_documents('data/NIST_EI_MS.msp')\n",
    "doc = next(spectra)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "numpy.ndarray"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc.peaks.mz)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}