{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "located-recording",
   "metadata": {},
   "source": [
    "# Train-test split\n",
    "Author: Michal Stary\n",
    "\n",
    "\n",
    "In this notebook we perform train-test split of our original datasets' files. \n",
    "Since we need to cope with multiple records of most of the compounds, such a split needs to be done carefuly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "indian-teddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to datasets\n",
    "mona_path = \"/storage/brno1-cerit/projects/msml/data/MoNA-export-GC-MS_Spectra.msp\"\n",
    "nist_path = \"/storage/brno6/home/xstary1/raims/data/20210925_NIST_EI_MS_cleaned.msp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "veterinary-accuracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from matchms.importing import load_from_msp\n",
    "from matchms.exporting import save_as_msp as save_as_msp_orig\n",
    "\n",
    "# REDEFINE save_as_msp function to avoid problems with nested comments\n",
    "def save_as_msp(ds, path):\n",
    "    for rec in ds:\n",
    "        if \"comments\" in rec.metadata:\n",
    "            rec.set(\"comment\", rec.get(\"comments\"))\n",
    "            rec._metadata.pop(\"comments\")\n",
    "    save_as_msp_orig(ds, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bored-jacket",
   "metadata": {},
   "outputs": [],
   "source": [
    "mona = [*load_from_msp(mona_path)]\n",
    "nist = [*load_from_msp(nist_path)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eight-person",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict()\n",
    "data[\"mona\"] = dict()\n",
    "data[\"nist\"] = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "typical-tampa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix seed for now\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-rotation",
   "metadata": {},
   "source": [
    "## Naive solution - Random split\n",
    "randomly assign into partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "congressional-guard",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(size, split1=.8, split2=.9):\n",
    "    split = int(size*split1)\n",
    "    split2 = int(size*split2)\n",
    "\n",
    "    ind = np.zeros(size)\n",
    "    ind[split:split2] = 1\n",
    "    ind[split2:] = 2\n",
    "\n",
    "    \n",
    "    np.random.shuffle(ind)\n",
    "\n",
    "    train_ind = np.where(ind==0)[0]\n",
    "    val_ind = np.where(ind==1)[0]\n",
    "    test_ind = np.where(ind==2)[0]\n",
    "\n",
    "    return train_ind, val_ind, test_ind\n",
    "\n",
    "def get_sub(docs, indcs):\n",
    "    return [docs[i] for i in indcs]\n",
    "\n",
    "def do_random_split(mona, nist):\n",
    "    data[\"mona\"][\"random_split_8_9\"] = [get_sub(mona, indcs) for indcs in random_split(len(mona))]\n",
    "    data[\"nist\"][\"random_split_8_9\"] = [get_sub(nist, indcs) for indcs in random_split(len(nist))]\n",
    "\n",
    "do_random_split(mona, nist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-platinum",
   "metadata": {},
   "source": [
    "### Notes\n",
    "Unfortunately, the simple naive solution does not take in account the repetitive records of the same compound in dataset. \n",
    "As a result, the effect of such a split is that compounds present in test set can overlap with compounds in train set. This violates the standard splitting conditions. \n",
    "\n",
    "SCENARIO: I may or may not have the compound in DB, but it was measured imperfecly in experiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-siemens",
   "metadata": {},
   "source": [
    "# Advanced solutions\n",
    "To simulate and assess performance without overlapping sets, we need to use records' attribute that represents what compound they represent.  \n",
    "\n",
    "Records without this attribute are skipped for now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acoustic-alabama",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**MONA***\n",
      "Dropped 6 out of 18898 records due to missing unique key\n",
      "Unique keys 9143\n",
      "\n",
      "***NIST***\n",
      "Dropped 60163 out of 350618 records due to missing unique key\n",
      "Unique keys 243413\n"
     ]
    }
   ],
   "source": [
    "# get mapping and first occurences of each unique compound\n",
    "def get_map(ds, unkey=\"inchikey\", parseuk_fn=lambda x: x):\n",
    "    map_ = dict()\n",
    "    dropped = 0\n",
    "    for i, rec in enumerate(ds):\n",
    "        if unkey not in rec.metadata or rec.metadata[unkey] == \"nan\":\n",
    "            dropped +=1\n",
    "            continue\n",
    "        \n",
    "        if parseuk_fn(rec.metadata[unkey]) not in map_:\n",
    "            map_[parseuk_fn(rec.metadata[unkey])] = [rec]\n",
    "        else:\n",
    "            map_[parseuk_fn(rec.metadata[unkey])].append(rec)\n",
    "            \n",
    "    print(f\"Dropped {dropped} out of {len(ds)} records due to missing unique key\")\n",
    "    return map_\n",
    "\n",
    "print(\"**MONA***\")\n",
    "mona_map = get_map(mona, \"inchikey\", lambda x: x.split(\"-\")[0])\n",
    "print(f\"Unique keys {len(mona_map)}\")\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"***NIST***\")\n",
    "nist_map = get_map(nist, \"inchikey\", lambda x: x.split(\"-\")[0])\n",
    "print(f\"Unique keys {len(nist_map)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-remove",
   "metadata": {},
   "source": [
    "## Put aside some compounds completely\n",
    "put aside few compunds for the very ultimate evaluation of working with previously unseen compounds\n",
    "\n",
    "\n",
    "We have 2 options how to do this:\n",
    "\n",
    "1. remove all records of selected compounds from datasets \n",
    "2. use dedicated extra dataset and make sure that its compounds are not in the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tender-briefing",
   "metadata": {},
   "source": [
    "### First approach - select & remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "assumed-jason",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random selection of compounds \n",
    "def select_random(ds_map, partition=.03):\n",
    "    split = int(len(ds_map)*partition)\n",
    "    ind = np.zeros(len(ds_map))\n",
    "    ind[split:] = 1\n",
    "    np.random.shuffle(ind)\n",
    "    \n",
    "    map_kept = dict()\n",
    "    map_left = dict()\n",
    "    for i, key in zip(ind, ds_map):\n",
    "        if i != 0:\n",
    "            map_kept[key] = ds_map[key]  \n",
    "        else:\n",
    "            map_left[key] = ds_map[key] \n",
    "            \n",
    "    return map_kept, map_left\n",
    "\n",
    "def do_select_random(mona_map, nist_map):\n",
    "    mona_map, mona_map_left = select_random(mona_map)\n",
    "    data[\"mona\"][\"left\"] = [[item for sublist in mona_map_left.values() for item in sublist]]\n",
    "\n",
    "    nist_map, nist_map_left = select_random(nist_map)\n",
    "    data[\"nist\"][\"left\"] = [[item for sublist in nist_map_left.values() for item in sublist]]\n",
    "\n",
    "    return mona_map, nist_map\n",
    "\n",
    "mona_map, nist_map = do_select_random(mona_map, nist_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-nightmare",
   "metadata": {},
   "source": [
    "### Second approach - obtain, check, (remove)\n",
    "We will use small hi-res datasets from RECETOX\n",
    "\n",
    "we can either remove mutual compounds from base datasets (MONA/NIST) or from these extra datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "suburban-poland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 0 out of 386 records due to missing unique key\n",
      "Dropped 0 out of 265 records due to missing unique key\n"
     ]
    }
   ],
   "source": [
    "# path to extra datasets\n",
    "rcx1_path = \"/storage/brno1-cerit/projects/msml/data/recetox_gc-ei_ms_20201028.msp\"\n",
    "rcx2_path = \"/storage/brno1-cerit/projects/msml/data/rcx_gc-orbitrap_metabolites_20210817.msp\"\n",
    "\n",
    "\n",
    "rcx1 = [*load_from_msp(rcx1_path)]\n",
    "rcx2 = [*load_from_msp(rcx2_path)]\n",
    "\n",
    "rcx1_map = get_map(rcx1)\n",
    "rcx2_map = get_map(rcx2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "satellite-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "# substract from ds1 all records that have same InChiKey as any record in ds2\n",
    "def substract_by_key(ds1_map, ds2_map):\n",
    "    ds1_attr = set(ds1_map.keys())\n",
    "    ds2_attr = set(ds2_map.keys())\n",
    "    left_attr = ds1_attr.difference(ds2_attr)    \n",
    "    #print(len(ds1_attr), len(left_attr))\n",
    "    return left_attr\n",
    "#rcx1[0].metadata[\"inchikey\"]\n",
    "def get_list_by_keys(keys, map_):\n",
    "    res = []\n",
    "    for key in keys:\n",
    "        res += map_[key]\n",
    "    return res\n",
    "\n",
    "def do_small():\n",
    "    data[\"mona\"][\"rcx1_no_mona\"] = [get_list_by_keys(substract_by_key(rcx1_map, mona_map), rcx1_map)]\n",
    "    data[\"mona\"][\"rcx2_no_mona\"] = [get_list_by_keys(substract_by_key(rcx2_map, mona_map), rcx2_map)]\n",
    "    \n",
    "    data[\"nist\"][\"rcx1_no_nist\"] = [get_list_by_keys(substract_by_key(rcx1_map, nist_map), rcx1_map)]\n",
    "    data[\"nist\"][\"rcx2_no_nist\"] = [get_list_by_keys(substract_by_key(rcx2_map, nist_map), rcx2_map)]\n",
    "do_small()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-secondary",
   "metadata": {},
   "source": [
    "# Scenario 2 - dealing solely with known compounds\n",
    "Firstly, we divide our dataset into train-val-test partitions to simulate prediction of missing parts of compounds that are part of our database.\n",
    "\n",
    "We do that by leveraging the duplicity of compounds in dababases:\n",
    "        \n",
    "    All first occurences of compounds in dataset are put into train set\n",
    "    Left out records are then splitted by compounds and put into validation and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afraid-yesterday",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub_nested(map_, indcs):\n",
    "    keys = np.array(list(map_.keys()))[indcs]\n",
    "    return [item for sublist in [map_[i] for i in keys] for item in sublist]\n",
    "\n",
    "def get_first_others(ds_map):\n",
    "    first = []\n",
    "    others_map = dict()\n",
    "    for key in ds_map:\n",
    "        first.append(ds_map[key][0])\n",
    "        if len(ds_map[key]) > 1:\n",
    "            others_map[key] = ds_map[key][1:]\n",
    "    return first, others_map\n",
    "\n",
    "def do_first_others_split(mona_map, nist_map):\n",
    "    mona_first, mona_others_map = get_first_others(mona_map)\n",
    "    nist_first, nist_others_map = get_first_others(nist_map)\n",
    "    \n",
    "    # split by compounds\n",
    "    mona_others_ind = random_split(len(mona_others_map.keys()), 0,.5)\n",
    "    nist_others_ind = random_split(len(nist_others_map.keys()), 0,.5)\n",
    "    \n",
    "    data[\"mona\"][\"in_database\"] = [mona_first] + [get_sub_nested(mona_others_map, indcs) for indcs in mona_others_ind][1:]\n",
    "    data[\"nist\"][\"in_database\"] = [nist_first] + [get_sub_nested(nist_others_map, indcs) for indcs in nist_others_ind][1:]\n",
    "\n",
    "do_first_others_split(mona_map, nist_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-myanmar",
   "metadata": {},
   "source": [
    "# Scenario 3 - dealing solely with unknown compounds\n",
    "Secodly, we divide our dataset into train-val-test partitions to simulate prediction of missing parts of compounds that are not in our database.\n",
    "\n",
    "    Dataset is splitted so either only one record per compound is kept or that repetetive records of the same compound are only in one partition. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuffed-bullet",
   "metadata": {},
   "source": [
    "## Keep unique only random split\n",
    "keeps only first occurence of each compound in ds\n",
    "\n",
    "randomly assign into partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "golden-planet",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_mona = random_split(len(mona_map))\n",
    "split_nist = random_split(len(nist_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "grateful-reaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_occ(ds_map):\n",
    "    first = []\n",
    "    for key in ds_map:\n",
    "        first.append(ds_map[key][0])\n",
    "    return first\n",
    "\n",
    "def do_first_only_random_split(mona_map, nist_map):\n",
    "    data[\"mona\"][\"unique_inchi\"] = [get_sub(get_first_occ(mona_map), indcs) for indcs in split_mona]\n",
    "    data[\"nist\"][\"unique_inchi\"] = [get_sub(get_first_occ(nist_map), indcs) for indcs in split_nist]\n",
    "\n",
    "do_first_only_random_split(mona_map, nist_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-briefing",
   "metadata": {},
   "source": [
    "## Duplicates in set only split\n",
    "assure that duplicate records of any compounds occurs only in one partition\n",
    "\n",
    "SCENARIO: I DO NOT have the compound in DB, but it was measured imperfecly in experiment and."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cleared-restriction",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def do_duplicates_in_partition_only_split(mona_map, nist_map):\n",
    "    data[\"mona\"][\"de_novo\"] = [get_sub_nested(mona_map, indcs) for indcs in split_mona]\n",
    "    data[\"nist\"][\"de_novo\"] = [get_sub_nested(nist_map, indcs) for indcs in split_nist]\n",
    "\n",
    "do_duplicates_in_partition_only_split(mona_map, nist_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-billy",
   "metadata": {},
   "source": [
    "## Inspections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "worthy-newfoundland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': '2,4-DINITROPHENOL',\n",
       " 'synon': '$:00in-source',\n",
       " 'db#': 'JP000002',\n",
       " 'inchikey': 'UFBJCMHMOXMLKC-UHFFFAOYSA-N',\n",
       " 'spectrum_type': 'MS1',\n",
       " 'instrument_type': 'EI-B',\n",
       " 'instrument': 'VARIAN MAT-44',\n",
       " 'ion_mode': 'P',\n",
       " 'formula': 'C6H4N2O5',\n",
       " 'mw': '184',\n",
       " 'exactmass': '184.01202122799998',\n",
       " 'smiles': '[O-1][N+1](=O)c(c1)cc([N+1]([O-1])=O)c(O)c1',\n",
       " 'inchi': 'InChI=1S/C6H4N2O5/c9-6-2-1-4(7(10)11)3-5(6)8(12)13/h1-3,9H',\n",
       " 'computed smiles': 'C1=C(C=C(C(=C1)O)N(=O)=O)N(=O)=O',\n",
       " 'accession': 'JP000002',\n",
       " 'date': '2016.01.19 (Created 2008.10.21, modified 2011.05.06)',\n",
       " 'author': 'KOGA M, UNIV. OF OCCUPATIONAL AND ENVIRONMENTAL HEALTH',\n",
       " 'license': 'CC BY-NC-SA',\n",
       " 'exact mass': '184.01202',\n",
       " 'ionization energy': '70 eV',\n",
       " 'ion type': '[M]+*',\n",
       " 'splash': 'splash10-0w33-9300000000-b1719d21caddddc047cb',\n",
       " 'submitter': 'University of Tokyo Team (Faculty of Engineering, University of Tokyo)',\n",
       " 'mona rating': '3.75',\n",
       " 'num peaks': '64'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mona[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "closing-cradle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['random_split_8_9', 'left', 'rcx1_no_nist', 'rcx2_no_nist', 'in_database', 'unique_inchi', 'de_novo'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"nist\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "detected-technique",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[280494, 35062, 35062]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[*map(len, data[\"nist\"][\"random_split_8_9\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sound-passage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[225611, 28108, 27980]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[*map(len,data[\"nist\"][\"de_novo\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "random-mailing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[236111, 22978, 22610]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[*map(len,data[\"nist\"][\"in_database\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "covered-efficiency",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n, k) tuples, where n=number of duplicates and k=number of compounds with k duplicates\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 210330),\n",
       " (2, 15066),\n",
       " (3, 5629),\n",
       " (4, 2847),\n",
       " (5, 1447),\n",
       " (6, 427),\n",
       " (7, 153),\n",
       " (9, 69),\n",
       " (8, 58),\n",
       " (10, 34),\n",
       " (11, 14),\n",
       " (12, 14),\n",
       " (13, 10),\n",
       " (14, 4),\n",
       " (19, 3),\n",
       " (15, 2),\n",
       " (17, 2),\n",
       " (24, 1),\n",
       " (16, 1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(\"(n, k) tuples, where n=number of duplicates and k=number of compounds with k duplicates\")\n",
    "Counter([len(nist_map[k]) for k in nist_map]).most_common()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-gathering",
   "metadata": {},
   "source": [
    "## Save into files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "young-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def save(data, path):\n",
    "    for ds in data:\n",
    "        #os.makedirs(path + ds, exist_ok=True)\n",
    "        for kind in data[ds]:\n",
    "            os.makedirs(f\"{path}/{ds}/{kind}\", exist_ok=True)\n",
    "            for part, recs in zip([\"train\", \"val\", \"test\"], data[ds][kind]):\n",
    "                save_as_msp(recs, f\"{path}/{ds}/{kind}/{part}.msp\")\n",
    "\n",
    "save(data, \"/storage/brno6/home/xstary1/raims/data\")\n",
    "#save(data, \"/storage/brno1-cerit/projects/msml/data/split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-transparency",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
